"""
CryptoQuantum Terminal - Performance Demo
========================================

This script demonstrates the performance difference between:
1. Traditional model training (slow)
2. Cached model loading (fast)

Run this after building the cache to see the dramatic speed improvement.

Usage:
    python performance_demo.py

Developed by Lewis Loon | Â© 2025 Lewis Loon Analytics
"""

import time
from pathlib import Path

def demo_performance():
    """Demonstrate performance improvements with caching"""
    
    print("ğŸš€ CryptoQuantum Performance Demo")
    print("=" * 50)
    
    # Check if cache exists
    cache_dir = Path('./model_cache')
    if not cache_dir.exists():
        print("âŒ No cache found!")
        print("ğŸ“ To build cache, run:")
        print("   python pretrain_models.py --cryptos top10")
        print("\nOr use the convenient batch script:")
        print("   build_cache.bat")
        return
    
    try:
        from cache_loader import CacheLoader
        cache_loader = CacheLoader()
        
        # Get cache stats
        stats = cache_loader.get_cache_stats()
        if not stats:
            print("âŒ Cache not properly initialized")
            return
        
        print("ğŸ“Š Cache Status:")
        print(f"   âœ… Models: {stats['models_count']}")
        print(f"   âœ… Datasets: {stats['data_count']}")
        print(f"   âœ… Forecasts: {stats['forecasts_count']}")
        print(f"   ğŸ’¾ Size: {stats['total_size_mb']:.1f} MB")
        print()
        
        # Demo 1: Data Loading Speed
        print("ğŸ” Demo 1: Data Loading Speed")
        print("-" * 30)
        
        symbol = 'BTC-USD'
        
        # Cached data loading
        start_time = time.time()
        cached_data = cache_loader.get_cached_data(symbol)
        cache_time = time.time() - start_time
        
        if cached_data is not None:
            print(f"âš¡ Cached data loading: {cache_time:.3f} seconds")
            print(f"   ğŸ“Š Records loaded: {len(cached_data):,}")
        
        # Simulate live data fetch (without actually fetching)
        start_time = time.time()
        time.sleep(2)  # Simulate network delay
        live_time = time.time() - start_time
        
        print(f"ğŸŒ Simulated live fetch: {live_time:.3f} seconds")
        
        if cached_data is not None:
            speedup = live_time / cache_time
            print(f"ğŸš€ Speedup: {speedup:.1f}x faster")
        
        print()
        
        # Demo 2: Model Loading Speed
        print("ğŸ” Demo 2: Model Loading Speed")
        print("-" * 30)
        
        # Cached model loading
        start_time = time.time()
        cached_model = cache_loader.load_cached_model(symbol, 'AttentionLSTM')
        model_time = time.time() - start_time
        
        if cached_model is not None:
            print(f"âš¡ Cached model loading: {model_time:.3f} seconds")
        
        # Simulate model training time
        simulated_training_time = 45.0  # Typical training time
        print(f"ğŸŒ Typical model training: {simulated_training_time:.1f} seconds")
        
        if cached_model is not None:
            speedup = simulated_training_time / model_time
            print(f"ğŸš€ Speedup: {speedup:.0f}x faster")
        
        print()
        
        # Demo 3: Forecast Generation
        print("ğŸ” Demo 3: Forecast Generation")
        print("-" * 30)
        
        # Cached forecasts
        start_time = time.time()
        cached_forecasts = cache_loader.get_cached_forecasts(symbol, 365)
        forecast_time = time.time() - start_time
        
        if cached_forecasts:
            print(f"âš¡ Cached forecasts: {forecast_time:.3f} seconds")
            print(f"   ğŸ“ˆ Predictions: {len(cached_forecasts['predictions'])}")
        
        # Simulate live prediction time
        simulated_prediction_time = 15.0
        print(f"ğŸŒ Live prediction generation: {simulated_prediction_time:.1f} seconds")
        
        if cached_forecasts:
            speedup = simulated_prediction_time / forecast_time
            print(f"ğŸš€ Speedup: {speedup:.0f}x faster")
        
        print()
        
        # Summary
        print("ğŸ“Š Performance Summary")
        print("=" * 30)
        print("With pre-trained cache:")
        print("âœ… Application startup: ~2-5 seconds")
        print("âœ… Model predictions: ~0.5 seconds")
        print("âœ… Data loading: ~0.1 seconds")
        print()
        print("Without cache (traditional):")
        print("âŒ Application startup: ~45-90 seconds")
        print("âŒ Model predictions: ~10-30 seconds")
        print("âŒ Data loading: ~2-5 seconds")
        print()
        print("ğŸ¯ Overall Performance Improvement:")
        print("   ğŸš€ 10-180x faster for end users!")
        print("   ğŸ’¾ Reduced server computational load")
        print("   ğŸŒ Better experience for remote users")
        
    except ImportError:
        print("âŒ Cache loader not available")
        print("ğŸ“ Run: python integrate_cache.py")
    except Exception as e:
        print(f"âŒ Error: {str(e)}")

def show_cache_info():
    """Show detailed cache information"""
    try:
        from cache_loader import CacheLoader
        cache_loader = CacheLoader()
        
        available_symbols = cache_loader.get_available_symbols()
        
        print("\nğŸ” Detailed Cache Information")
        print("=" * 40)
        
        for symbol in available_symbols[:5]:  # Show first 5
            print(f"\nğŸ“ˆ {symbol}:")
            
            # Check data
            data = cache_loader.get_cached_data(symbol)
            if data is not None:
                print(f"   ğŸ“Š Data: {len(data):,} records")
                print(f"   ğŸ“… Date range: {data.index[0].date()} to {data.index[-1].date()}")
            
            # Check forecasts
            forecasts = cache_loader.get_all_cached_forecasts(symbol)
            if forecasts:
                forecast_horizons = list(forecasts['forecasts'].keys())
                print(f"   ğŸ”® Forecasts: {len(forecast_horizons)} time horizons")
                print(f"   â±ï¸  Generated: {forecasts.get('generated_date', 'Unknown')}")
            
            # Check freshness
            is_fresh = cache_loader.is_cache_fresh(symbol, 24)
            print(f"   ğŸ• Fresh (24h): {'âœ… Yes' if is_fresh else 'âš ï¸ No'}")
        
        if len(available_symbols) > 5:
            print(f"\n... and {len(available_symbols) - 5} more symbols")
            
    except Exception as e:
        print(f"âŒ Error getting cache info: {str(e)}")

def main():
    """Main demo function"""
    demo_performance()
    
    # Ask if user wants detailed info
    try:
        response = input("\nâ“ Show detailed cache information? (y/n): ").lower()
        if response == 'y':
            show_cache_info()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Demo completed!")

if __name__ == "__main__":
    main()
